"""
G√©n√©rateur de rapport HTML pour l'analyse des extractions d'horaires.

Ce module g√©n√®re des rapports HTML d√©taill√©s √† partir des donn√©es stock√©es
en base de donn√©es, avec support des comparaisons d'horaires.
"""

import base64
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Tuple

import polars as pl
from jinja2 import Environment, FileSystemLoader

from core.ErrorHandler import ErrorCategory, ErrorSeverity, handle_errors
from core.Logger import create_logger

# Initialize logger for this module
logger = create_logger(
    module_name="GenererRapportHTML",
)


def to_json(value) -> Optional[str]:
    """
    Convertit une valeur en cha√Æne JSON encod√©e en base64.

    Cette fonction prend une valeur de n'importe quel type et la transforme en cha√Æne JSON,
    puis l'encode en base64 pour √©viter les probl√®mes d'√©chappement dans les templates HTML.

    Args:
        value: La valeur √† convertir et encoder

    Returns:
        str: Cha√Æne JSON encod√©e en base64, ou None si la valeur d'entr√©e est None
    """
    if value is None:
        return None

    try:
        # Si c'est d√©j√† une cha√Æne JSON valide, la retourner
        if isinstance(value, str):
            # V√©rifier si c'est un JSON valide
            json.loads(value)
            json_str = value
        else:
            # Sinon, essayer de convertir en JSON avec ensure_ascii=False pour pr√©server les caract√®res UTF-8
            json_str = json.dumps(value, ensure_ascii=False, indent=2)

        # Encoder en base64 en s'assurant que l'UTF-8 est pr√©serv√©
        return base64.b64encode(json_str.encode("utf-8")).decode("ascii")
    except Exception as e:
        logger.warning(f"Erreur conversion JSON pour template: {e}")
        # En cas d'erreur, convertir en cha√Æne puis encoder en base64
        json_str = json.dumps(str(value), ensure_ascii=False, indent=2)
        return base64.b64encode(json_str.encode("utf-8")).decode("ascii")


@handle_errors(
    category=ErrorCategory.UNKNOWN,
    severity=ErrorSeverity.HIGH,
    user_message="Erreur lors de la g√©n√©ration du rapport HTML",
)
def generer_rapport_html(
    db_file: str, table_name: str, titre_rapport: str, model_info: Optional[Dict] = None
) -> Tuple[str, str]:
    """
    G√©n√®re un rapport HTML complet √† partir des donn√©es de la base SQLite.

    Args:
        db_file: Chemin vers le fichier de base de donn√©es SQLite
        table_name: Nom de la table √† analyser
        titre_rapport: Titre du rapport
        model_info: Informations sur le mod√®le utilis√©

    Returns:
        Tuple contenant (r√©sum√©_html, chemin_fichier_html)

    Raises:
        FileNotFoundError: Si le fichier de base de donn√©es n'existe pas
        RuntimeError: Si les templates ne sont pas trouv√©s
    """
    try:
        logger.info(f"G√©n√©ration rapport HTML depuis: {Path(db_file).name}")

        # V√©rification de l'existence de la base de donn√©es
        db_path = Path(db_file)
        if not db_path.exists():
            raise FileNotFoundError(f"Base de donn√©es non trouv√©e: {db_file}")

        # Configuration du moteur de templates
        assets_dir = Path(__file__).parent.parent / "assets"
        if not assets_dir.exists():
            raise FileNotFoundError(f"R√©pertoire assets non trouv√©: {assets_dir}")

        env = Environment(loader=FileSystemLoader(str(assets_dir)))
        env.filters["tojson"] = to_json

        try:
            template = env.get_template("ReportTemplate.html")
            simple_template = env.get_template("SimpleReportTemplate.html")
            logger.debug("Templates charg√©s avec succ√®s")
        except Exception as e:
            raise RuntimeError(f"Erreur chargement templates: {e}")

        # Extraction des donn√©es depuis la base de donn√©es
        donnees_urls = _extract_data_from_database(db_file)
        logger.info(f"Donn√©es extraites: {len(donnees_urls)} enregistrements")

        # Traitement des donn√©es
        _process_data(donnees_urls)

        # Calcul des statistiques
        stats_globales = _calculate_global_stats(donnees_urls)
        statuts_disponibles = _group_by_status(donnees_urls)
        types_lieu_stats = _calculate_type_stats(donnees_urls)
        codes_http_stats = _calculate_http_stats(donnees_urls)

        # Pr√©paration des donn√©es pour le template
        donnees_template = {
            "titre_rapport": titre_rapport,
            "date_generation": datetime.now().strftime("%d/%m/%Y √† %H:%M"),
            "stats_globales": stats_globales,
            "statuts_disponibles": statuts_disponibles,
            "types_lieu_stats": types_lieu_stats,
            "codes_http_stats": codes_http_stats,
            "model_info": model_info,
        }

        # G√©n√©ration des rapports
        try:
            resume_html = simple_template.render(**donnees_template)
            html_content = template.render(**donnees_template)
            logger.debug("Templates rendus avec succ√®s")
        except Exception as e:
            raise RuntimeError(f"Erreur rendu template: {e}")

        # Sauvegarde du fichier
        fichier_rapport_html = _save_report(html_content)

        logger.info(f"Rapport g√©n√©r√© avec succ√®s: {fichier_rapport_html}")
        return resume_html, fichier_rapport_html

    except Exception as e:
        logger.error(f"Erreur g√©n√©ration rapport: {e}")
        raise


def _extract_data_from_database(db_file: str) -> list:
    """Extrait les donn√©es depuis la base de donn√©es."""
    try:
        uri = f"sqlite:///{db_file}"
        query = """
        SELECT 
            l.type_lieu, 
            l.identifiant, 
            l.nom, 
            l.url, 
            l.horaires_data_gl,
            r.statut_url AS statut, 
            r.message_url AS message, 
            r.llm_horaires_json, 
            r.llm_horaires_osm, 
            r.code_http,
            r.horaires_identiques,
            r.differences_horaires
        FROM resultats_extraction AS r 
        JOIN lieux AS l ON r.lieu_id = l.identifiant 
        ORDER BY l.identifiant
        """

        df = pl.read_database_uri(query=query, uri=uri, engine="connectorx")
        return df.to_dicts()

    except Exception as e:
        logger.error(f"Erreur extraction donn√©es: {e}")
        raise RuntimeError(f"Erreur lors de l'extraction des donn√©es: {e}")


def _process_data(donnees_urls: list) -> None:
    """Traite et normalise les donn√©es extraites."""
    for url in donnees_urls:
        try:
            # Convertir llm_horaires_json en objet Python si c'est une cha√Æne JSON
            if "llm_horaires_json" in url and url["llm_horaires_json"]:
                if isinstance(url["llm_horaires_json"], str):
                    try:
                        url["llm_horaires_json"] = json.loads(url["llm_horaires_json"])
                    except json.JSONDecodeError:
                        logger.warning(
                            f"JSON invalide pour {url.get('identifiant', 'inconnu')}"
                        )

            # Cr√©er le champ comparaison_horaires pour compatibilit√© avec le template
            _create_comparison_field(url)

            # S'assurer que les champs requis existent avec des valeurs par d√©faut
            _set_default_fields(url)

        except Exception as e:
            logger.warning(
                f"Erreur traitement donn√©es pour {url.get('identifiant', 'inconnu')}: {e}"
            )


def _create_comparison_field(url: dict) -> None:
    """Cr√©e le champ de comparaison pour le template."""
    horaires_identiques = url.get("horaires_identiques")
    differences_horaires = url.get("differences_horaires", "")

    if horaires_identiques is None:
        url["comparaison_horaires"] = "Non compar√©"
    elif horaires_identiques is True:
        url["comparaison_horaires"] = "IDENTIQUE - Aucune diff√©rence d√©tect√©e"
    elif horaires_identiques is False:
        url["comparaison_horaires"] = f"DIFF√âRENT - {differences_horaires}"
    else:
        url["comparaison_horaires"] = (
            differences_horaires if differences_horaires else "Erreur de comparaison"
        )


def _set_default_fields(url: dict) -> None:
    """D√©finit les valeurs par d√©faut pour les champs requis."""
    defaults = {
        "llm_horaires_json": None,
        "llm_horaires_osm": None,
        "horaires_data_gl": None,
        "code_http": 0,
        "message": "",
        "url": "",
        "nom": "",
        "identifiant": "",
        "type_lieu": "",
    }

    for field, default_value in defaults.items():
        url.setdefault(field, default_value)


def _calculate_global_stats(donnees_urls: list) -> dict:
    """Calcule les statistiques globales."""
    total_urls = len(donnees_urls)

    # Statistiques de comparaison
    comparisons_done = len(
        [u for u in donnees_urls if u.get("horaires_identiques") is not None]
    )
    comparisons_identical = len(
        [u for u in donnees_urls if u.get("horaires_identiques") is True]
    )
    comparisons_different = len(
        [u for u in donnees_urls if u.get("horaires_identiques") is False]
    )
    comparisons_not_done = total_urls - comparisons_done

    return {
        "total_urls": total_urls,
        "comparisons_done": comparisons_done,
        "comparisons_identical": comparisons_identical,
        "comparisons_different": comparisons_different,
        "comparisons_not_done": comparisons_not_done,
    }


def _group_by_status(donnees_urls: list) -> list:
    """Groupe les URLs par statut avec 4 cat√©gories distinctes."""
    # Configuration des statuts avec 4 cat√©gories
    statuts_config = {
        "success": {
            "nom": "Succ√®s",
            "emoji": "‚úÖ",
            "type": "success",
            "description": "URLs accessibles avec horaires OSM extraits et comparaison r√©ussie (horaires identiques)",
        },
        "schedule_diff": {
            "nom": "Diff√©rences horaires",
            "emoji": "‚ö†Ô∏è",
            "type": "warning",
            "description": "URLs accessibles avec horaires extraits mais diff√©rences d√©tect√©es lors de la comparaison",
        },
        "access_error": {
            "nom": "Erreurs d'acc√®s",
            "emoji": "üîí",
            "type": "error",
            "description": "URLs inaccessibles, codes d'erreur HTTP, probl√®mes de connexion ou contenu indisponible",
        },
        "extraction_error": {
            "nom": "Erreurs d'extraction",
            "emoji": "‚ùå",
            "type": "error",
            "description": "URLs accessibles mais √©chec de l'extraction LLM ou de la conversion OSM",
        },
    }

    # Classification selon les nouveaux crit√®res
    for url in donnees_urls:
        # Crit√®re 1: V√©rifier l'accessibilit√© de l'URL
        url_accessible = url.get("statut") == "ok" and url.get("code_http", 0) in range(
            200, 300
        )

        # Crit√®re 2: V√©rifier la pr√©sence d'horaires OSM extraits
        has_osm_hours = (
            url.get("llm_horaires_osm")
            and url["llm_horaires_osm"].strip()
            and not url["llm_horaires_osm"].startswith("Erreur")
        )

        # Crit√®re 3: V√©rifier le r√©sultat de la comparaison
        comparison_result = url.get("horaires_identiques")

        # Classification hi√©rarchique
        if not url_accessible:
            # Probl√®me d'accessibilit√© : codes HTTP non-200, URL invalide, etc.
            url["statut"] = "access_error"
        elif not has_osm_hours:
            # URL accessible mais √©chec extraction/conversion
            url["statut"] = "extraction_error"
        elif comparison_result is True:
            # URL accessible, extraction r√©ussie, horaires identiques
            url["statut"] = "success"
        elif comparison_result is False:
            # URL accessible, extraction r√©ussie, mais horaires diff√©rents
            url["statut"] = "schedule_diff"
        else:
            # URL accessible, extraction r√©ussie, mais comparaison impossible/non effectu√©e
            url["statut"] = "extraction_error"

    # Regrouper par statut dans l'ordre de priorit√©
    statuts_disponibles = []
    ordre_statuts = ["success", "schedule_diff", "extraction_error", "access_error"]

    for statut_code in ordre_statuts:
        config = statuts_config[statut_code]
        urls_du_statut = [u for u in donnees_urls if u["statut"] == statut_code]
        if urls_du_statut:
            statuts_disponibles.append(
                {
                    "code": statut_code,
                    "nom": config["nom"],
                    "emoji": config["emoji"],
                    "type": config["type"],
                    "description": config["description"],
                    "count": len(urls_du_statut),
                    "urls": urls_du_statut,
                }
            )

    return statuts_disponibles


def _calculate_type_stats(donnees_urls: list) -> list:
    """Calcule les statistiques par type de lieu."""
    type_counts = {}
    for url in donnees_urls:
        type_lieu = url.get("type_lieu", "Non d√©fini")
        type_counts[type_lieu] = type_counts.get(type_lieu, 0) + 1

    return [
        {"type": type_lieu, "count": count} for type_lieu, count in type_counts.items()
    ]


def _calculate_http_stats(donnees_urls: list) -> list:
    """Calcule les statistiques par code HTTP."""
    code_counts = {}
    for url in donnees_urls:
        code_http = url.get("code_http", 0)
        code_counts[code_http] = code_counts.get(code_http, 0) + 1

    return sorted(
        [{"code": code, "count": count} for code, count in code_counts.items()],
        key=lambda x: x["code"],
    )


def _save_report(html_content: str) -> str:
    """Sauvegarde le rapport HTML et retourne le nom du fichier."""
    try:
        fichier_rapport_html = (
            f"Rapport_SmartWatch_{datetime.now().strftime('%Y%m%d_%H%M')}.html"
        )

        with open(fichier_rapport_html, "w", encoding="utf-8") as f:
            f.write(html_content)

        logger.debug(f"Rapport sauvegard√©: {fichier_rapport_html}")
        return fichier_rapport_html

    except Exception as e:
        logger.error(f"Erreur sauvegarde rapport: {e}")
        raise RuntimeError(f"Erreur lors de la sauvegarde: {e}")


def main():
    """Exemple d'utilisation et tests."""
    logger.section("TEST G√âN√âRATION RAPPORT")

    try:
        script_dir = Path(__file__).parent.parent / "data"
        nom_fic = "alerte_modif_horaire_lieu_devstral"
        db_file = script_dir / f"{nom_fic}.db"

        if not db_file.exists():
            logger.error(f"Base de donn√©es de test non trouv√©e: {db_file}")
            return

        model_info = {
            "modele": "test-model",
            "base_url": "http://localhost:8000",
            "fournisseur": "TEST",
        }

        resume_html, fichier_html = generer_rapport_html(
            db_file=str(db_file),
            table_name="resultats_extraction",
            titre_rapport="Rapport de test",
            model_info=model_info,
        )

        logger.info(f"Test r√©ussi - rapport g√©n√©r√©: {fichier_html}")

    except Exception as e:
        logger.error(f"Erreur lors du test: {e}")


if __name__ == "__main__":
    main()
